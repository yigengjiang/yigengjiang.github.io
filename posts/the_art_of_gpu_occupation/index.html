<!DOCTYPE html>
<html lang="zh-CN"></html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>The Art of GPU Occupation - Yigeng’s Blog</title>
    <meta property="og:title" content="The Art of GPU Occupation - Yigeng’s Blog">
    
    <meta name="twitter:card" content="summary">
    
      
      <meta property="description" content="I have been developing 2030 project almost more than one month.
[&amp;hellip;] I primarily focus on analysing and cleaning data. Recently, I&amp;rsquo;ve delved into some advanced work about LLM. &amp;hellip;">
      <meta property="og:description" content="I have been developing 2030 project almost more than one month.
[&amp;hellip;] I primarily focus on analysing and cleaning data. Recently, I&amp;rsquo;ve delved into some advanced work about LLM. &amp;hellip;">
      
    

    
    
    

    

    

    
    <link rel="stylesheet" href="/css/style.css">
    
    <link rel="stylesheet" href="/css/fonts.css">
    
    <link rel="stylesheet" href="/css/custom.css">
    
    



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  </head>

  
  
  
  <body class="single posts">
    <div class="crop-h"></div><div class="crop-v"></div><div class="crop-c"></div>
    <nav class="nav-top small">
    <div class="logo">
    
      <a href="/">
      
      至繁归于至简
      
      </a>
    
    </div>
    <div class="menu"><span><a href="/">Home</a></span>
      <span><a href="/about/">About</a></span>
      <span class="active"><a href="/posts/">Archives</a></span>
      <span><a href="/categories">Categories</a></span>
      <span><a href="/search/">Search</a></span>
      <span><a href="/series/">Series</a></span>
      <span><a href="/tags/">Tags</a></span>
      <span><a href="/index.xml">RSS</a></span>
      
    </div>
    </nav>

<div class="article-meta">
<h1 class="title">The Art of GPU Occupation</h1>

<h3 class="meta-line">
  <span>

<span class="author">yigeng</span>






<span class="date">2023-08-25</span>


</span>
  <span class="term">
  
  
  {<a href="/categories/develop/" class="term-cat">Develop</a>}
  
  
  
  
  
  <a href="/tags/thoughts/" class="term-tag">[thoughts,</a>
  
  <a href="/tags/python/" class="term-tag">Python,</a>
  
  <a href="/tags/gpu/" class="term-tag">GPU]</a>
  
  
  </span>
</h3>
</div>

<div class="main">




<p>I have been developing <a href="https://informatics.xmu.edu.cn/info/1053/28969.htm">2030 project</a> almost more than one month.</p>
<p>I primarily focus on analysing and cleaning data. Recently, I&rsquo;ve delved into some advanced work about <a href="https://en.wikipedia.org/wiki/Large_language_model">LLM</a>. Specifically, I use <a href="https://arxiv.org/abs/2106.09685">LoRA</a> to fine-tune <a href="https://github.com/LinkSoul-AI/Chinese-Llama-2-7b">Chinese-Llama-2-7b</a>.</p>
<p>In the beginning, five postgraduates were working on similar tasks as mine on different dataset. Sometimes, serval Ph.D students also use the same server. However, we only have 8 NVIDIA A800 on the server for fine-tuning LLM.</p>
<p>It is quite annoying to see available GPUs slip away simply because we were slightly slow in executing commands. Therefore, efficiently securing enough GPUs under limited resources becomes crucial.</p>
<p>In summary, my goal is to design a Python program that can help me grab available GPUs and once I acquire $n$ GPUs, then start running the real program.</p>
<p>Before delving into the solution, I&rsquo;d like to share something else. Creating a program to grab GPUs aligns with the first of the <a href="https://en.wikipedia.org/wiki/Larry_Wall#Virtues_of_a_programmer">Three Virtues of a Programmer</a> — Laziness. This is because automated GPU allocation saves me more time and energy compared to do it manually. Moreover, automation enhances efficiency and accuracy. The process of automating tasks is also a great way to hone my programming skills.</p>
<p>Now, let&rsquo;s delve into the source codes and briefly discuss the main idea.</p>
<p>To grab GPUs, we first need to gather the information about GPUs. For this, I leverage the Python libaray <code>subprocess</code> to execute the  <code>nvidia-smi</code> command, which provides details about GPU status. The <code>get_gpu_mem</code> function retrieves the memory of a specified GPU while <code>get_free_gpus</code> returns available GPUs as a list.</p>
<pre><code class="language-python">def get_gpu_mem(gpu_id):
    gpu_query = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'])
    gpu_memory = [int(x) for x in gpu_query.decode('utf-8').split('\n')[:-1]]
    return gpu_memory[gpu_id]

def get_free_gpus()-&gt;list:
    gpu_query = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'])
    gpu_memory = [int(x) for x in gpu_query.decode('utf-8').split('\n')[:-1]]
    free_gpus = [i for i, mem in enumerate(gpu_memory) if mem &lt; 100]
    return free_gpus
</code></pre>
<p>So, how to occupy available GPUs ? I  employ Python&rsquo;s <code>multiprocessing</code> library to achieve this. If there are $n$ avaible GPUs, $n$ subprocesses will their own GPU.</p>
<p>In the main process <code>for-loop</code>, the update rate of the <code>occupy_num</code>  variable lags far behind the actual code execution. As a result, the <code>occupy_all_gpus</code> function spawns numerous subprocesses. In fact, the total number of subprocesses exceeds $n$ . However, thanks to the <code>Lock </code> mechanism, only $n$ subprocesses get to occupy GPUs and grab GPUs orderly.</p>
<p>To occupy a GPU essentially means claiming its memory. In the <code>occupy_gpu</code> function, I generate a high-dimensional torch tensor on the designated GPU and then make the subprocess enter a sleep state.</p>
<pre><code class="language-python">def occupy_gpu(gpu_id:int, n, occupy_num, ocpy_gpus, lock, a_dim=100000):
    with lock:
        if get_gpu_mem(gpu_id) &lt; 100 and occupy_num.value &lt; n:
            import torch
            a = torch.ones((a_dim,a_dim)).cuda(gpu_id)
            ocpy_gpus[occupy_num.value]= gpu_id
            occupy_num.value += 1
            print(f&quot;Occupying GPU {gpu_id}, Total Occupied: {occupy_num.value}&quot;)
    while True:
        time.sleep(10)

def occupy_all_gpus(n:int, occupy_num, ocpy_gpus, interval=10):
    print(&quot;Launching process to occupy GPU ...&quot;)
    lock = Lock()
    processes = [] #List to store the processes
    while occupy_num.value &lt; n:
        free_gpus = get_free_gpus()
        will_occupy_num = min(n, max(0,len(free_gpus)))
        for i in range(will_occupy_num):
            if occupy_num.value &lt; n:
                p = Process(target=occupy_gpu, args=(free_gpus[i], n, occupy_num, ocpy_gpus, lock))
                p.start()
                processes.append(p)
        time.sleep(interval) # enough time to occupy gpus and update nvidia-smi
    return processes, ocpy_gpus
        
</code></pre>
<p>With that, I conclude the introduction to the mechanism of occupying GPUs ends. Once we&rsquo;ve occupy $n$ GPUs, it`s time to run our real program. However, before that, we need to terminate all the subprocesses.</p>
<pre><code class="language-python">def run_my_program(n, desired_script, processes, ocpy_gpus, occupy_num):
    for p in processes:
        p.terminate()
    ocpy_gpus_list = list(ocpy_gpus[:occupy_num.value])
    cuda_visible_devices = &quot;,&quot;.join(map(str, ocpy_gpus_list))
    os.environ['CUDA_VISIBLE_DEVICES'] = cuda_visible_devices
    subprocess.run([desired_script, str(n)])
</code></pre>
<p>In a nutshell, the core of my solution is employing Python multiprocessing to occupy GPUs memory.</p>
<p>The source code is available for download <a href="https://typora-bookworm-images.oss-cn-hangzhou.aliyuncs.com/grab_gpu.py">here</a>. I developed it using Python 3.11. You can run the script by executing the following command.</p>
<pre><code class="language-bash">python grab_gpu.py --n 3 --otime 30 --spath ./train.sh
</code></pre>
<p>I finish the whole work from programming to polish this blog by the help of chatGPT. The capabilities of this tool have profoundly transformed my academic and personal life. The more I engage with it, the more I feel can&rsquo;t live without it.</p>
<p>This evokes mixed feelings. While I&rsquo;m elated witnessing the moumental strides AI is making to better our lives, the sheer potency of AI instills a lingering apprehension that one day, AI might spiral out of our control.&#x1f914;</p>
<p>The emergence of tools like chatGPT prompts reflection on topics such as the essence of human learning and the evolving nature of a programmer&rsquo;s role.</p>
<p>Recently, I&rsquo;ve been reading <a href="https://book.douban.com/subject/11609943/"><em>The Art of Unix Programming</em></a>.&quot; Inspired by its title, I&rsquo;ve chosen to name my blog <em>The Art of GPU Occupation</em>.&#x1f601;</p>
<p>Hope this blog can help you and if you have any questions or insights, I welcome a hearty discussion! &#x1f606;</p>



<nav class="post-nav fullwidth kai">
  <span class="nav-prev">&larr; <a href="/posts/zhixin-academy-2023summer/">癸卯年志心青年立心营纪录与感想</a></span>
  <span class="nav-next"><a href="/posts/higher_education_of_computer_science/">信息学科的大学之道</a> &rarr;</span>
</nav>


<section class="fullwidth comments">
    <div class="comments">
        <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "yigeng" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </div>
</section>


<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/math-code.js" defer></script>
<script src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>

<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.js" defer></script>
</div>
  <footer class="small">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/rstudio/markdown/inst/resources/prism-xcode.css">
<script src="//cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/copy-button.min.js" defer></script>
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@xiee/utils/css/copy-button.min.css">
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/key-buttons.min.js" defer></script>
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@xiee/utils/css/key-buttons.min.css">
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/external-link.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/fullwidth.min.js" defer></script>






  
  
  <hr/>
  
  <p class="nav-bottom">
    <span>© <a href="https://github.com/yigengjiang">Yigeng Jiang</a> 2023-2024 | <a href="https://github.com/yigengjiang">Github</a></span>
    <span class="menu-bottom">







<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="Attribution-NonCommercial-ShareAlike 4.0 International">License</a>
<a href="/search/">Search</a>
<a href="#">Back to top</a>
</span>
  </p>
  
  </footer>
  </body>
</html>



