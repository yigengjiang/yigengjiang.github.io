<!DOCTYPE html>
<html lang="zh-CN"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script></html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <title>KV Cache in Transformer - Yigengâ€™s Blog</title>
    <meta property="og:title" content="KV Cache in Transformer - Yigengâ€™s Blog">
    
    <meta name="twitter:card" content="summary">
    
      
      <meta property="description" content="æœ€è¿‘åœ¨ç ”ç©¶Transformerï¼Œç»å¸¸ç¢°åˆ°KV-Cacheï¼Œæ¯æ¬¡éƒ½è¦å»æŸ¥ä¸€ä¸‹æ‰èƒ½å›æƒ³èµ·æ¥è¿™ä¸ªä¸œè¥¿ã€‚ä½†ç¨å¾®ç ”ç©¶äº†ä¸€ä¸‹ï¼Œè§‰å¾—kv-cacheä»æ•°å­¦ä¸Šæ¥è¯´æ˜¯éå¸¸æ˜¾ç„¶çš„ï¼Œåœ¨è¿™é‡Œæ€»ç»“ä¸€ä¸‹æˆ‘å¯¹kv-cacheçš„ç†è§£ã€‚
[&amp;hellip;] KV-Cacheæºè‡ªäºTransformerï¼Œå…¨ç§°æ˜¯Key-Value Cacheï¼Œä¹Ÿå°±æ˜¯å¯¹Keyå’ŒValueçš„ç¼“å­˜ã€‚
[&amp;hellip;] Transformerç›¸å¯¹ &amp;hellip;">
      <meta property="og:description" content="æœ€è¿‘åœ¨ç ”ç©¶Transformerï¼Œç»å¸¸ç¢°åˆ°KV-Cacheï¼Œæ¯æ¬¡éƒ½è¦å»æŸ¥ä¸€ä¸‹æ‰èƒ½å›æƒ³èµ·æ¥è¿™ä¸ªä¸œè¥¿ã€‚ä½†ç¨å¾®ç ”ç©¶äº†ä¸€ä¸‹ï¼Œè§‰å¾—kv-cacheä»æ•°å­¦ä¸Šæ¥è¯´æ˜¯éå¸¸æ˜¾ç„¶çš„ï¼Œåœ¨è¿™é‡Œæ€»ç»“ä¸€ä¸‹æˆ‘å¯¹kv-cacheçš„ç†è§£ã€‚
[&amp;hellip;] KV-Cacheæºè‡ªäºTransformerï¼Œå…¨ç§°æ˜¯Key-Value Cacheï¼Œä¹Ÿå°±æ˜¯å¯¹Keyå’ŒValueçš„ç¼“å­˜ã€‚
[&amp;hellip;] Transformerç›¸å¯¹ &amp;hellip;">
      
    

    
    
    
    <meta name="twitter:image" content="https://miro.medium.com/v2/resize:fit:875/0*sexO6adGhaKr7aH0.gif">
    
    

    

    

    
    <link rel="stylesheet" href="/css/style.css">
    
    <link rel="stylesheet" href="/css/fonts.css">
    
    <link rel="stylesheet" href="/css/custom.css">
    
    



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  </head>

  
  
  
  <body class="single posts">
    <div class="crop-h"></div><div class="crop-v"></div><div class="crop-c"></div>
    <nav class="nav-top small">
    <div class="logo">
    
      <a href="/">
      
      <img src="/images/bird.jpg" alt="è‡³ç¹å½’äºè‡³ç®€" />
      
      </a>
    
    </div>
    <div class="menu"><span><a href="/">Home</a></span>
      <span><a href="/about/">About</a></span>
      <span class="active"><a href="/posts/">Archives</a></span>
      <span><a href="/categories">Categories</a></span>
      <span><a href="/search/">Search</a></span>
      <span><a href="/series/">Series</a></span>
      <span><a href="/tags/">Tags</a></span>
      <span><a href="/index.xml">RSS</a></span>
      
    </div>
    </nav>

<div class="article-meta">
<h1 class="title">KV Cache in Transformer</h1>

<h3 class="meta-line">
  <span>

<span class="author">yigeng</span>






<span class="date">2024-05-25</span>


</span>
  <span class="term">
  
  
  {<a href="/categories/machine-learning/" class="term-cat">Machine Learning</a>}
  
  
  
  
  
  <a href="/tags/transformer/" class="term-tag">[Transformer,</a>
  
  <a href="/tags/python/" class="term-tag">Python]</a>
  
  
  </span>
</h3>
</div>

<div class="main">




<p>æœ€è¿‘åœ¨ç ”ç©¶Transformerï¼Œç»å¸¸ç¢°åˆ°KV-Cacheï¼Œæ¯æ¬¡éƒ½è¦å»æŸ¥ä¸€ä¸‹æ‰èƒ½å›æƒ³èµ·æ¥è¿™ä¸ªä¸œè¥¿ã€‚ä½†ç¨å¾®ç ”ç©¶äº†ä¸€ä¸‹ï¼Œè§‰å¾—kv-cacheä»æ•°å­¦ä¸Šæ¥è¯´æ˜¯éå¸¸æ˜¾ç„¶çš„ï¼Œåœ¨è¿™é‡Œæ€»ç»“ä¸€ä¸‹æˆ‘å¯¹kv-cacheçš„ç†è§£ã€‚</p>
<p>KV-Cacheæºè‡ªäºTransformerï¼Œå…¨ç§°æ˜¯Key-Value Cacheï¼Œä¹Ÿå°±æ˜¯å¯¹Keyå’ŒValueçš„ç¼“å­˜ã€‚</p>
<p>Transformerç›¸å¯¹äºCNNæœ€å¤§çš„ç‰¹ç‚¹å°±æ˜¯å¼•å…¥äº†Attentionè®¡ç®—ï¼Œå…¶è®¡ç®—å…¬å¼å¦‚ä¸‹:
$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V \tag{1}
$$
å…¶ä¸­$Q,K,V$åˆ†åˆ«ä»£è¡¨Queryã€Keyå’ŒValueçŸ©é˜µï¼Œ$d_k$ä»£è¡¨Scaling factorï¼Œå±äºå®æ•°ã€‚æ‰€è°“KV-Cacheä¾¿æ˜¯åœ¨Attentionè®¡ç®—ä¸­ï¼Œé€šè¿‡å¯¹Keyå’ŒValueè¿›è¡Œç¼“å­˜ï¼Œä»è€Œå‡å°ä¸å¿…è¦çš„å¼€é”€ï¼Œè¿™é‡Œâ€œä¸å¿…è¦â€æŒ‡ç”±äºæˆ‘ä»¬å·²ç»ç¼“å­˜äº†éƒ¨åˆ†Keyå’ŒValueï¼Œå› æ­¤ä¸éœ€è¦å†è¿›è¡Œé‡å¤è®¡ç®—ä»¥å¾—åˆ°è¿™éƒ¨åˆ†Keyå’ŒValueï¼Œä½¿ç”¨ä¹‹å‰è®¡ç®—å¥½äº†çš„å³å¯ã€‚</p>
<p>é‚£ä¹ˆä¸ºä»€ä¹ˆè¯´åç»­çš„Attentionè®¡ç®—ä¸­ä¼šç”¨åˆ°ä¹‹å‰çš„Keyå’ŒValueå‘¢ï¼Ÿè¿™å°±è¦è®²ä¸€ä¸‹Transformeræ˜¯å¦‚ä½•è¿›è¡Œæ¨ç†äº†ã€‚</p>
<p><img src="https://miro.medium.com/v2/resize:fit:875/0*sexO6adGhaKr7aH0.gif" alt="auto-regressive generation of the decoder">
<em>In the auto-regressive generation of the decoder, given an input the model predicts the next token, and then taking the combined input in the next step the next prediction is made. (Image source: <a href="https://jalammar.github.io/illustrated-gpt2/)">https://jalammar.github.io/illustrated-gpt2/)</a>.</em></p>
<p>å¦‚ä¸Šå›¾æ‰€ç¤ºGPT-2ä½¿ç”¨çš„æ˜¯Transformerçš„decoderæ¶æ„ï¼Œè¿™ç±»æ¨¡å‹åœ¨æ¨ç†æ—¶é‡‡å–çš„æ˜¯auto-regressiveè‡ªå›å½’å¼çš„é£æ ¼ï¼Œå…·ä½“çš„è¯´ï¼Œåœ¨ç¬¬$i$ä¸ªroundæ—¶ï¼Œæ¨¡å‹è¾“å‡º1ä¸ªtokenï¼Œä¾‹å¦‚ä¸Šå›¾ä¸­çš„â€œrobotâ€ã€‚</p>
<p>åˆ°äº†ç¬¬$i+1$ä¸ª$\mathrm{round}$æ¨¡å‹ä¼šç»§ç»­æ¨ç†ï¼Œä½†è¿™æ—¶æ¨¡å‹çš„è¾“å…¥ä¼šå‘ç”Ÿå˜åŒ–ï¼Œå®ƒä¼šå°†ä¸Šä¸ª$\mathrm{round}$é¢„æµ‹å¾—åˆ°çš„tokenï¼Œ&ldquo;robot&rdquo; appendåˆ°ç¬¬$i$ä¸ª$\mathrm{round}$çš„è¾“å…¥<code>recite ... A</code>çš„åé¢ï¼Œä½œä¸ºç¬¬$i+1$ä¸ª$\mathrm{round}$æ¨¡å‹è¾“å…¥ã€‚</p>
<p>è¿™é‡Œå¯ä»¥ç¨å¾®çœ‹ä¸‹Transformeræ¨ç†çš„æºç ï¼Œauto-regressiveçš„ä½“ç°ä¾¿æ˜¯åœ¨<code>torch.cat</code>å‡½æ•°ã€‚</p>
<pre><code class="language-python"># Generate the translation word by word
while decoder_input.size(1) &lt; seq_len:
    # build mask for target and calculate output
    decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int).type_as(source_mask).to(device)
    out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)

    # project next token
    prob = model.project(out[:, -1])
    _, next_word = torch.max(prob, dim=1)
    decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1)

    # print the translated word
    print(f&quot;{tokenizer_tgt.decode([next_word.item()])}&quot;, end=' ')

    # break if we predict the end of sentence token
    if next_word == tokenizer_tgt.token_to_id('[EOS]'):
        break
</code></pre>
<p>ä¸€å¥è¯æ€»ç»“ï¼Œauto-regressiveç±»æ¨¡å‹ä¼šå°†é¢„æµ‹çš„ç»“æœConcatenateåˆ°è¾“å…¥æœ«å°¾ä½œä¸ºæ–°çš„è¾“å…¥ï¼Œç„¶åç»§ç»­é¢„æµ‹ã€‚</p>
<p>äº†è§£äº†Transformeræ˜¯å¦‚ä½•æ¨ç†çš„ï¼Œæˆ‘ä»¬å†å›åˆ°Attentionçš„è®¡ç®—ã€‚</p>
<p>å‡è®¾ä»$\mathrm{round_1}$å¼€å§‹æ¨¡å‹è¾“å…¥æ˜¯1ä¸ªtokenï¼Œåœ¨å…·ä½“çš„å®ç°ä¸Šï¼Œæˆ‘ä»¬ä¼šç”¨Vectorå‘é‡æ¥è¡¨ç¤º1ä¸ªtokenï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨cvä¸­ç”¨çŸ©é˜µè¡¨ç¤ºå›¾åƒé‚£æ ·ï¼Œæˆ‘ä»¬è®°è¿™ä¸ªtokenè¾“å…¥å‘é‡ä¸º$x_1$ï¼Œä¸ºäº†æ–¹ä¾¿åç»­æ•°å­¦è¡¨è¾¾ï¼Œå½“æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‘é‡æ—¶ï¼Œé»˜è®¤å®ƒä¸ºè¡Œå‘é‡ï¼ŒåŒºåˆ«äºä¼ ç»Ÿçº¿ä»£ä¸­é»˜è®¤å‘é‡ä¸ºåˆ—å‘é‡ã€‚</p>
<p>è¿™é‡Œé¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œåœ¨Transformerçš„ä»£ç å®ç°ä¸Šï¼ŒAttentionæœºåˆ¶ä¸­çš„$Q_w,K_w,V_wâ€¨$æ˜¯ä¸€ç»„å¯å­¦ä¹ çš„æƒé‡å‚æ•°ï¼Œç”¨<code>nn.Linear</code>æ¥è¡¨ç¤ºçš„çº¿æ€§å±‚ã€‚</p>
<p>åœ¨$\mathrm{round_1}$æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†$x_1$åˆ†åˆ«å’Œ$Q_w,K_w,V_wâ€¨$ç›¸ä¹˜å¾—åˆ° $q_1,k_1,v_1$ï¼Œç„¶åå¸¦å…¥åˆ°ç­‰å¼1ä¸­å®ŒæˆAttentionçš„è®¡ç®—ã€‚æ•´ä¸ªæ¨¡å‹forwardç»“æŸï¼Œä¼šå¾—åˆ°æ¨¡å‹è¾“å‡ºçš„1ä¸ªtokenï¼Œæˆ‘ä»¬æŠŠå®ƒè®°ä¸º$x_2$ã€‚</p>
<p>åœ¨$\mathrm{round_2}$æ—¶ï¼Œæˆ‘ä»¬å°†$x_1, x_2$ç»„åˆä½œä¸ºè¾“å…¥ï¼Œå’Œ$Q_w,K_w,V_wâ€¨$åšçŸ©é˜µä¹˜å¾—åˆ°$q_1, q_2$,$k_1,k_2$ä»¥åŠ$v_1 v_2â€¨$ã€‚</p>
<p>åœ¨$\mathrm{round_n}$æ—¶ï¼Œæˆ‘ä»¬æ‰‹é‡Œæœ‰$x_1,x_2,&hellip;x_n$ï¼Œè®°$X=\begin{pmatrix}x_1\\x_2\\ \vdots\\x_n\end{pmatrix}$ã€‚æˆ‘ä»¬å°†$X$åˆ†åˆ«ä¸$Q_w,K_w,V_wâ€¨$ç›¸ä¹˜å¾—åˆ°$Q,K,V$</p>
<p>æœ‰äº†$Q,K,V$ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹Attentionçš„è®¡ç®—ï¼Œæ³¨æ„Attentionè®¡ç®—çš„å…³é”®åœ¨äºQKVçŸ©é˜µä¹˜ï¼Œsoftmaxå¯¹æ¯ä¸€è¡Œå…ƒç´ åšå½’ä¸€åŒ–ï¼ŒScaling factorå¯¹çŸ©é˜µä¸­æ¯ä¸ªå…ƒç´ çš„valueåšæ”¾ç¼©ï¼Œæ€»çš„æ¥è¯´softmaxå’ŒScaling factoréƒ½åªæ˜¯å¯¹çŸ©é˜µä¸­å…ƒç´ å€¼åšæ”¾ç¼©ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿½ç•¥softmaxå’ŒScaling factorå¹¶ä¸ä¼šå¯¹KV-Cacheçš„ç†è§£æœ‰å½±å“ã€‚é‚£ä¹ˆï¼ŒAttentionçš„è®¡ç®—å±•å¼€æˆå‘é‡å½¢å¼å¦‚ä¸‹ï¼š
$$
\mathrm{Attention}(Q,K,V) \approx \begin{pmatrix}q_1 \\q_2 \\ \vdots\\q_n\end{pmatrix}\begin{pmatrix}k^T_1, k^T_2,\cdots, k^T_n\end{pmatrix}\begin{pmatrix}v_1 \\v_2 \ \vdots\\v_n\end{pmatrix}\\
=\begin{pmatrix}q_1k^T_1&amp;&amp;&amp;\\q_2k^T_1&amp;q_2k^T_2&amp;&amp;\\ \vdots&amp;\vdots&amp;\ddots&amp;\\q_nk^T_1&amp;q_nk^T_2&amp;\cdots&amp;q_nk^T_n\end{pmatrix}\begin{pmatrix}v_1 \\v_2 \\ \vdots\\v_n\end{pmatrix}\\
=\begin{pmatrix}q_1k^T_1v_1 \\q_2k^T_1v_1 + q_2k^T_2v_2 \\ \vdots\\q_nk^T_1v_1 + q_nk^T_2v_2+\cdots+q_nk^T_nv_n\end{pmatrix} \tag{2}
$$
æ³¨æ„åœ¨ä¸Šé¢ç¬¬äºŒæ­¥æ¨å¯¼ä¸­ï¼Œ$QK^T$æ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼Œå› ä¸ºåœ¨$\mathrm{round_i}$æ—¶ï¼Œæˆ‘ä»¬ä»…æœ‰$q_j,k_j,v_j$å…¶ä¸­$j \leq i$ã€‚ä»¤$A$è¡¨ç¤ºå¼å­2ä¸­çš„æœ€ç»ˆç»“æœï¼Œ$A$çš„ç¬¬$i$ä¸ªè¡Œå‘é‡$A_i=\sum_{j}^{i}q_ik_j^Tv_j$ã€‚</p>
<p>çœ‹åˆ°è¿™é‡Œï¼Œä¸çŸ¥é“ä½ æœ‰æ²¡æœ‰ä¸€ç§æç„¶å¤§æ‚Ÿçš„æ„Ÿè§‰ğŸ˜‰ã€‚ç”±äºæˆ‘ä»¬åœ¨$\mathrm{round_{j &lt; i}}$çš„è¿‡ç¨‹ä¸­ï¼Œç¼“å­˜äº†$k_j,v_j$ï¼Œé‚£ä¹ˆåœ¨$\mathrm{round_i}$æ—¶æˆ‘ä»¬åªéœ€è¦å†è®¡ç®—ä¸€ä¸‹$k_i,v_i$ï¼Œå¤ç”¨ç¼“å­˜çš„keyå’Œvalueä¾¿èƒ½å®Œæˆæ•´ä¸ªAttentionçš„è®¡ç®—ã€‚</p>
<p>è¿™é‡Œå¯ä»¥å†å¤šæ‰¯ä¸€ä¸‹ï¼ŒKV-Cacheçš„<strong>å”¯ä¸€ä½œç”¨</strong>ä¾¿æ˜¯é¿å…å†—ä½™è®¡ç®—ã€‚åœ¨Transformerçš„è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯ä¸å­˜åœ¨KV-cacheçš„ï¼Œå› ä¸ºåœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªå¥å­çš„å‰n-1ä¸ªtokenå–‚ç»™æ¨¡å‹ï¼Œå°†é¢„æµ‹å¾—åˆ°çš„tokenå’Œground truthåšcross-entropyï¼Œä¹Ÿå°±æ˜¯è¯´åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯one-shoté£æ ¼è€Œä¸æ˜¯è‡ªå›å½’å¼çš„ï¼Œè®­ç»ƒä»£ç ä¸ºè¯ã€‚</p>
<pre><code class="language-python">for i, batch in enumerate(iterator):
    src = batch.src
    trg = batch.trg

    optimizer.zero_grad()
    output = model(src, trg[:, :-1])
    output_reshape = output.contiguous().view(-1, output.shape[-1])
    trg = trg[:, 1:].contiguous().view(-1)

    loss = criterion(output_reshape, trg)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
    optimizer.step()
</code></pre>
<p>KV-caheä»…å­˜åœ¨äºæ¨ç†é˜¶æ®µï¼Œè€Œä¸”åªå­˜åœ¨äºdecoderä¸­ã€‚ä¸‹å›¾æ˜¯Transformerç»“æ„ï¼Œåœ¨å³ä¾§decoderä¸­ï¼ŒMulti-Head Attentionæ¨¡å—çš„Keyå’ŒValueæ¥è‡ªäºEncoderçš„è¾“å‡ºï¼Œæ˜¯ä¸€æ¬¡æ€§å…¨éƒ¨ç”Ÿæˆå¥½äº†çš„ï¼Œå› æ­¤æˆ‘ä»¬é€šå¸¸æ˜¯åœ¨è®©decoderé¢„æµ‹å‰åœ¨å†…å­˜ä¸­ç¼“å­˜å¥½encoderçš„outputï¼Œä½œä¸ºKeyå’ŒValueç”¨äºcross-Attentionçš„è®¡ç®—ã€‚è€Œdecoderä¸­çš„Masked Multi-Head Attentionæ¨¡å—ä¾¿æ˜¯ä¸Šæ–‡æ‰€è®²ï¼Œåœ¨é€tokençš„ç”Ÿæˆä¸­ä¸æ–­ç¼“å­˜key-valueã€‚</p>
<p><img src="https://typora-bookworm-images.oss-cn-hangzhou.aliyuncs.com/images/202405250937880.png" alt="The full model architecture of the transformer">
<em>The full model architecture of the transformer. (Image source: Fig 1 &amp; 2 in Vaswani, et al., 2017.)</em></p>
<h1 id="references">References</h1>
<ol>
<li><a href="https://medium.com/@joaolages/kv-caching-explained-276520203249">Transformers KV Caching Explained</a></li>
<li><a href="https://github.com/hkproj/pytorch-transformer">pytorch-transformer</a></li>
<li>Ashish Vaswani, et al. <a href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">â€œAttention is all you need.â€</a> NIPS 2017.</li>
<li><a href="https://github.com/hyunwoongko/transformer">Transformer: PyTorch Implementation of &ldquo;Attention Is All You Need&rdquo;</a></li>
</ol>



<nav class="post-nav fullwidth kai">
  <span class="nav-prev">&larr; <a href="/posts/2023_summary/">2023 å®‰æ²³æ¡¥</a></span>
  <span class="nav-next"><a href="/posts/design_of_metagpt/">The Design of MetaGPT</a> &rarr;</span>
</nav>


<section class="fullwidth comments">
    <div class="comments">
        <div id="disqus_thread"></div>
<script>
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "yigeng" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </div>
</section>


<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/math-code.js" defer></script>
<script src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>

<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.js" defer></script>
</div>
  <footer class="small">
  <script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/center-img.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/alt-title.min.js" defer></script>
<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/rstudio/markdown/inst/resources/prism-xcode.css">
<script src="//cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/copy-button.min.js" defer></script>
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@xiee/utils/css/copy-button.min.css">
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/key-buttons.min.js" defer></script>
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@xiee/utils/css/key-buttons.min.css">
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/heading-anchor.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/external-link.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/ol-id.min.js" defer></script>
<script src="//cdn.jsdelivr.net/npm/@xiee/utils/js/fullwidth.min.js" defer></script>






  
  
  <hr/>
  
  <p class="nav-bottom">
    <span>Â© <a href="https://github.com/yigengjiang">Yigeng Jiang</a> 2023-2025 | <a href="https://github.com/yigengjiang">Github</a></span>
    <span class="menu-bottom">







<a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="Attribution-NonCommercial-ShareAlike 4.0 International">License</a>
<a href="/search/">Search</a>
<a href="#">Back to top</a>
</span>
  </p>
  
  </footer>
  </body>
</html>



